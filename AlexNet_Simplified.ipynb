{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import os\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (28709, 48, 48) (28709,)\n",
      "Valid set (3589, 48, 48) (3589,)\n",
      "Test set (3589, 48, 48) (3589,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'fer2013.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    \n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Valid set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (28709, 48, 48, 1) (28709, 7)\n",
      "Validation set (3589, 48, 48, 1) (3589, 7)\n",
      "Test set (3589, 48, 48, 1) (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "image_size = 48\n",
    "num_labels = 7\n",
    "num_channels = 1\n",
    "\n",
    "def shuffle_data(dataset, labels):\n",
    "    assert len(dataset) == len(labels)\n",
    "    p = np.random.permutation(len(dataset))\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = shuffle_data(train_dataset, train_labels)\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_conv(x, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Variable, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "        affn:      whether to affn-transform outputs\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n",
    "    \"\"\"\n",
    "\n",
    "    phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n",
    "    n_out = int(x.get_shape()[3])\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n",
    "                       name='beta', trainable=True, dtype=x.dtype)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n",
    "                        name='gamma', trainable=True, dtype=x.dtype)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    def mean_var_with_update():\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = tf.cond(phase_train,\n",
    "                                      mean_var_with_update,\n",
    "                                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "def batch_norm_fc(x, phase_train):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 2D BD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Variable, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "        affn:      whether to affn-transform outputs\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n",
    "    \"\"\"\n",
    "\n",
    "    phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n",
    "    n_out = int(x.get_shape()[1])\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n",
    "                       name='beta', trainable=True, dtype=x.dtype)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n",
    "                        name='gamma', trainable=True, dtype=x.dtype)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9, name='ema')\n",
    "\n",
    "    def mean_var_with_update():\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = tf.cond(phase_train,\n",
    "\n",
    "                                      mean_var_with_update,\n",
    "                                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "c1_kernel = 5\n",
    "c2_kernel = 5\n",
    "c3_kernel = 5\n",
    "\n",
    "c1_depth = 32\n",
    "c2_depth = 32\n",
    "c3_depth = 64\n",
    "#c4_depth = 256\n",
    "fc1_nodes = 3072\n",
    "\n",
    "beta = 10\n",
    "starter_learning_rate = 7e-4\n",
    "weight_scale = 1e-2\n",
    "with tf.device('/gpu:0'):\n",
    "    alexnet_simple = tf.Graph()\n",
    "\n",
    "    with alexnet_simple.as_default():\n",
    "        #Input data\n",
    "        tf_dataset = tf.placeholder(dtype=tf.float32, \n",
    "            shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_labels = tf.placeholder(dtype=tf.float32,\n",
    "            shape=(batch_size, num_labels))\n",
    "\n",
    "        phase_train = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "          # Variables.\n",
    "        c1_weights = weight_scale * tf.Variable(tf.truncated_normal(\n",
    "          [c1_kernel, c1_kernel, num_channels, c1_depth], stddev=0.1))\n",
    "        c1_bias = tf.Variable(tf.zeros([c1_depth]))\n",
    "\n",
    "        c2_weights = weight_scale * tf.Variable(tf.truncated_normal(\n",
    "            [c2_kernel, c2_kernel, c1_depth, c2_depth], stddev=0.1))\n",
    "        c2_bias = tf.Variable(tf.zeros([c2_depth]))\n",
    "\n",
    "        c3_weights = weight_scale * tf.Variable(tf.truncated_normal(\n",
    "            [c3_kernel, c3_kernel, c2_depth, c3_depth], stddev=0.1))\n",
    "        c3_bias = tf.Variable(tf.zeros([c3_depth]))\n",
    "\n",
    "        #c4_weights = tf.Variable(tf.truncated_normal(\n",
    "        #    [c3_kernel, c3_kernel, c3_depth, c4_depth], stddev=0.1))\n",
    "        #c4_bias = tf.Variable(tf.zeros([c4_depth]))\n",
    "\n",
    "        fc1_weights = weight_scale * tf.Variable(tf.truncated_normal(\n",
    "                    [image_size //8 * image_size //8 * c3_depth, fc1_nodes], stddev=0.1))\n",
    "        fc1_bias = tf.Variable(tf.zeros([fc1_nodes]))\n",
    "\n",
    "\n",
    "        hidden_weights = weight_scale * tf.Variable(tf.truncated_normal(\n",
    "                [fc1_nodes, num_labels], stddev=0.1))\n",
    "        hidden_bias = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        def model(data, phase_train):\n",
    "            with tf.variable_scope('conv1'):\n",
    "                conv = tf.nn.conv2d(data, c1_weights, [1,1,1,1], padding='SAME') + c1_bias\n",
    "                conv = batch_norm_conv(conv, phase_train)\n",
    "                hidden = tf.nn.relu(conv)\n",
    "                pooled = tf.nn.max_pool(hidden, ksize=[1,3,3,1],\n",
    "                                        strides=[1,2,2,1], padding='SAME')\n",
    "            #pooled_norm = tf.nn.local_response_normalization(pooled)\n",
    "\n",
    "            with tf.variable_scope('conv2'):\n",
    "                conv = tf.nn.conv2d(pooled, c2_weights, [1,1,1,1], padding='SAME') + c2_bias\n",
    "                conv = batch_norm_conv(conv, phase_train)\n",
    "                hidden = tf.nn.relu(conv)\n",
    "                pooled = tf.nn.avg_pool(hidden, ksize=[1,3,3,1],\n",
    "                                        strides=[1,2,2,1], padding='SAME')\n",
    "            #pooled_norm = tf.nn.local_response_normalization(pooled)\n",
    "\n",
    "            with tf.variable_scope('conv3'):\n",
    "                conv = tf.nn.conv2d(pooled, c3_weights, [1,1,1,1], padding='SAME') + c3_bias\n",
    "                conv = batch_norm_conv(conv, phase_train)\n",
    "                hidden = tf.nn.relu(conv)\n",
    "                pooled = tf.nn.avg_pool(hidden, ksize=[1,3,3,1],\n",
    "                                        strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "            #conv = tf.nn.conv2d(hidden, c4_weights, [1,1,1,1], padding='SAME')\n",
    "            #hidden = tf.nn.relu(conv + c4_bias)\n",
    "\n",
    "            #hidden_norm = tf.nn.local_response_normalization(hidden)\n",
    "\n",
    "            shape = pooled.get_shape().as_list()\n",
    "\n",
    "            reshape = tf.reshape(pooled, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "            fc1 = tf.matmul(reshape, fc1_weights) + fc1_bias\n",
    "            fc1 = batch_norm_fc(fc1, phase_train)\n",
    "            #fc1 = tf.nn.relu(fc1)\n",
    "            #fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "            hidden = tf.cond(keep_prob > 0.1, lambda: tf.nn.dropout(tf.nn.relu(fc1), keep_prob=keep_prob),\n",
    "                                             lambda: tf.nn.relu(fc1))\n",
    "\n",
    "            return tf.matmul(hidden, hidden_weights) + hidden_bias           \n",
    "\n",
    "        logits = model(tf_dataset, phase_train)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_labels))\n",
    "\n",
    "        l2_loss = beta * ( tf.nn.l2_loss(c1_weights) + tf.nn.l2_loss(c1_bias) +\n",
    "                       tf.nn.l2_loss(c2_weights) + tf.nn.l2_loss(c2_bias) +\n",
    "                       tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_bias) +\n",
    "                       tf.nn.l2_loss(hidden_weights) + tf.nn.l2_loss(hidden_bias) +\n",
    "                       tf.nn.l2_loss(c3_weights) + tf.nn.l2_loss(c3_bias) \n",
    "                       #tf.nn.l2_loss(c4_weights) + tf.nn.l2_loss(c4_bias)  \n",
    "                     )\n",
    "\n",
    "        # global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "        #                                           1000, 0.96, staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(starter_learning_rate).minimize(loss+l2_loss)\n",
    "\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        acc_placeholder = tf.placeholder(dtype=tf.float32)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", acc_placeholder)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Restore from:  ./checkpoints/tang/tang-10000\n",
      "Minibatch loss at step 10000: 0.722211\n",
      "Minibatch accuracy: 84.0%\n",
      "------ Validation accuracy: 38.0%\n",
      "Minibatch loss at step 10500: 1.242278\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 36.0%\n",
      "Minibatch loss at step 11000: 1.255164\n",
      "Minibatch accuracy: 48.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 11500: 1.250318\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 46.0%\n",
      "Minibatch loss at step 12000: 1.029070\n",
      "Minibatch accuracy: 68.0%\n",
      "------ Validation accuracy: 44.0%\n",
      "Minibatch loss at step 12500: 1.203859\n",
      "Minibatch accuracy: 58.0%\n",
      "------ Validation accuracy: 38.0%\n",
      "Minibatch loss at step 13000: 1.164301\n",
      "Minibatch accuracy: 62.0%\n",
      "------ Validation accuracy: 40.0%\n",
      "Minibatch loss at step 13500: 0.998527\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 14000: 1.077410\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 52.0%\n",
      "Minibatch loss at step 14500: 1.214168\n",
      "Minibatch accuracy: 62.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 15000: 1.076183\n",
      "Minibatch accuracy: 64.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 15500: 1.031761\n",
      "Minibatch accuracy: 58.0%\n",
      "------ Validation accuracy: 46.0%\n",
      "Minibatch loss at step 16000: 1.207633\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 16500: 1.139090\n",
      "Minibatch accuracy: 62.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 17000: 1.056909\n",
      "Minibatch accuracy: 56.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 17500: 1.159771\n",
      "Minibatch accuracy: 56.0%\n",
      "------ Validation accuracy: 56.0%\n",
      "Minibatch loss at step 18000: 0.959779\n",
      "Minibatch accuracy: 60.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 18500: 1.079845\n",
      "Minibatch accuracy: 56.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 19000: 0.917533\n",
      "Minibatch accuracy: 70.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 19500: 1.108969\n",
      "Minibatch accuracy: 68.0%\n",
      "------ Validation accuracy: 40.0%\n",
      "Minibatch loss at step 20000: 1.433376\n",
      "Minibatch accuracy: 46.0%\n",
      "------ Validation accuracy: 52.0%\n",
      "Minibatch loss at step 20500: 1.496911\n",
      "Minibatch accuracy: 58.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 21000: 1.036881\n",
      "Minibatch accuracy: 64.0%\n",
      "------ Validation accuracy: 48.0%\n",
      "Minibatch loss at step 21500: 0.977033\n",
      "Minibatch accuracy: 76.0%\n",
      "------ Validation accuracy: 46.0%\n",
      "Minibatch loss at step 22000: 1.080775\n",
      "Minibatch accuracy: 58.0%\n",
      "------ Validation accuracy: 50.0%\n",
      "Minibatch loss at step 22500: 0.905024\n",
      "Minibatch accuracy: 50.0%\n",
      "------ Validation accuracy: 54.0%\n",
      "Minibatch loss at step 23000: 0.725532\n",
      "Minibatch accuracy: 84.0%\n",
      "------ Validation accuracy: 44.0%\n",
      "Minibatch loss at step 23500: 0.994807\n",
      "Minibatch accuracy: 62.0%\n",
      "------ Validation accuracy: 54.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 24001\n",
    "model_name = 'tang'\n",
    "train_writer = tf.summary.FileWriter('./summary/'+ model_name+'/train', graph=alexnet_simple)\n",
    "\n",
    "valid_writer = tf.summary.FileWriter('./summary/'+model_name+ '/valid', graph=alexnet_simple)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "\n",
    "with tf.Session(graph=alexnet_simple) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  initial_step = 0\n",
    "\n",
    "  ## Load saved checkpoint\n",
    "  ckpt = tf.train.get_checkpoint_state(\n",
    "    os.path.dirname('./checkpoints/' + model_name + '/checkpoint'))\n",
    "\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    # Restore from checkpoint\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "    print(\"Restore from: \", ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    try:\n",
    "        os.mkdir('./checkpoints/' + model_name)\n",
    "    except:\n",
    "        print(\"It's ok\")\n",
    "    \n",
    "  def validate(val_dataset, val_labels):\n",
    "    num_iterations = val_dataset.shape[0] / batch_size\n",
    "    \n",
    "    predict_arr = None\n",
    "    labels_arr = None\n",
    "    \n",
    "    for step in range(num_iterations):\n",
    "        offset = (step * batch_size) % (val_dataset.shape[0] - batch_size)\n",
    "        batch_data = val_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = val_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels, phase_train: False, keep_prob: 0}\n",
    "        \n",
    "        predictions, = session.run(\n",
    "          [prediction], feed_dict=feed_dict)\n",
    "        if predict_arr is None:\n",
    "            predict_arr = predictions\n",
    "            labels_arr = batch_labels\n",
    "        else:\n",
    "            np.append(predict_arr, predictions, 0)\n",
    "            np.append(labels_arr, batch_labels, 0)\n",
    "    return accuracy(predict_arr, labels_arr)    \n",
    "        \n",
    "           \n",
    "  #bar = progressbar.ProgressBar(redirect_stdout=True)\n",
    "  for step in range(initial_step, num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels, phase_train: True, keep_prob: 0.8}\n",
    "    _, l, predictions, = session.run(\n",
    "      [optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    #if (step > 0 and step % 100 == 0):\n",
    "    #  summ = session.run([summary], feed_dict=feed_dict)\n",
    "    #  train_writer.add_summary(valid_summ, step)\n",
    "    #  train_writer.add_summary(train_summ, step)\n",
    "    \n",
    "    if (step % 100 == 0):  \n",
    "      #summ = session.run([merged], feed_dict=feed_dict)\n",
    "      train_writer.add_summary(loss_summary.eval(feed_dict=feed_dict), step)\n",
    "        \n",
    "    if (step % 200 == 0):  \n",
    "        \n",
    "      #valid_predict = session.run([valid_prediction], feed_dict=feed_dict)\n",
    "      train_acc = accuracy(predictions, batch_labels)\n",
    "      valid_acc = validate(valid_dataset[:100], valid_labels[:100])\n",
    "    \n",
    "      train_writer.add_summary(acc_summary.eval(feed_dict={acc_placeholder: valid_acc}), step)\n",
    "      valid_writer.add_summary(acc_summary.eval(feed_dict={acc_placeholder: train_acc}), step)\n",
    "     \n",
    "    if (step % 500 == 0):  \n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % train_acc)\n",
    "      print('------ Validation accuracy: %.1f%%' % valid_acc)\n",
    "    if (step % 1000 == 0):\n",
    "        saver.save(session, './checkpoints/' + model_name + '/' + model_name, global_step=step)\n",
    "        \n",
    "    #bar.update(float(step-initial_step) / (num_steps - initial_step) * 100)\n",
    "        \n",
    "  #test_predict = session.run([test_prediction], feed_dict=feed_dict)\n",
    "  print('====== Test accuracy: %.1f%%' % validate(test_dataset, test_labels))\n",
    "  #saver.save(session, './checkpoints/' + model_name, global_step=num_steps)\n",
    "  session.close()\n",
    "\n",
    "\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
